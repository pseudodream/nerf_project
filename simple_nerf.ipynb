{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/airalcorn2/pytorch-nerf/blob/master/66bdbc812bd0a196e194052f3f12cb2e.npz?raw=True\" -O 66bdbc812bd0a196e194052f3f12cb2e.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE9MrEIp9acd",
        "outputId": "9de17fd1-65a9-4835-c24e-9204cc783f37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 21:26:20--  https://github.com/airalcorn2/pytorch-nerf/blob/master/66bdbc812bd0a196e194052f3f12cb2e.npz?raw=True\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/airalcorn2/pytorch-nerf/raw/master/66bdbc812bd0a196e194052f3f12cb2e.npz [following]\n",
            "--2024-04-29 21:26:20--  https://github.com/airalcorn2/pytorch-nerf/raw/master/66bdbc812bd0a196e194052f3f12cb2e.npz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/airalcorn2/pytorch-nerf/master/66bdbc812bd0a196e194052f3f12cb2e.npz [following]\n",
            "--2024-04-29 21:26:21--  https://raw.githubusercontent.com/airalcorn2/pytorch-nerf/master/66bdbc812bd0a196e194052f3f12cb2e.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24103428 (23M) [application/octet-stream]\n",
            "Saving to: ‘66bdbc812bd0a196e194052f3f12cb2e.npz’\n",
            "\n",
            "66bdbc812bd0a196e19 100%[===================>]  22.99M  48.5MB/s    in 0.5s    \n",
            "\n",
            "2024-04-29 21:26:23 (48.5 MB/s) - ‘66bdbc812bd0a196e194052f3f12cb2e.npz’ saved [24103428/24103428]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "def get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os):\n",
        "    # Sample depths (t_is_c). See Equation (2) in Section 4.\n",
        "    u_is_c = torch.rand(*list(ds.shape[:2]) + [N_c]).to(ds)\n",
        "    t_is_c = t_i_c_bin_edges + u_is_c * t_i_c_gap\n",
        "    # Calculate the points along the rays (r_ts_c) using the ray origins (os), sampled\n",
        "    # depths (t_is_c), and ray directions (ds). See Section 4: r(t) = o + t * d.\n",
        "    r_ts_c = os[..., None, :] + t_is_c[..., :, None] * ds[..., None, :]\n",
        "    return (r_ts_c, t_is_c)\n",
        "\n",
        "\n",
        "def get_fine_query_points(w_is_c, N_f, t_is_c, t_f, os, ds):\n",
        "    # See text surrounding Equation (5) in Section 5.2 and:\n",
        "    # https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html#discrete_distributions.\n",
        "\n",
        "    # Define PDFs (pdfs) and CDFs (cdfs) from weights (w_is_c).\n",
        "    w_is_c = w_is_c + 1e-5\n",
        "    pdfs = w_is_c / torch.sum(w_is_c, dim=-1, keepdim=True)\n",
        "    cdfs = torch.cumsum(pdfs, dim=-1)\n",
        "    cdfs = torch.cat([torch.zeros_like(cdfs[..., :1]), cdfs[..., :-1]], dim=-1)\n",
        "\n",
        "    # Get uniform samples (us).\n",
        "    us = torch.rand(list(cdfs.shape[:-1]) + [N_f]).to(w_is_c)\n",
        "\n",
        "    # Use inverse inverse transform sampling to sample the depths (t_is_f).\n",
        "    idxs = torch.searchsorted(cdfs, us, right=True)\n",
        "    t_i_f_bottom_edges = torch.gather(t_is_c, 2, idxs - 1)\n",
        "    idxs_capped = idxs.clone()\n",
        "    max_ind = cdfs.shape[-1]\n",
        "    idxs_capped[idxs_capped == max_ind] = max_ind - 1\n",
        "    t_i_f_top_edges = torch.gather(t_is_c, 2, idxs_capped)\n",
        "    t_i_f_top_edges[idxs == max_ind] = t_f\n",
        "    t_i_f_gaps = t_i_f_top_edges - t_i_f_bottom_edges\n",
        "    u_is_f = torch.rand_like(t_i_f_gaps).to(os)\n",
        "    t_is_f = t_i_f_bottom_edges + u_is_f * t_i_f_gaps\n",
        "\n",
        "    # Combine the coarse (t_is_c) and fine (t_is_f) depths and sort them.\n",
        "    (t_is_f, _) = torch.sort(torch.cat([t_is_c, t_is_f.detach()], dim=-1), dim=-1)\n",
        "    # Calculate the points along the rays (r_ts_f) using the ray origins (os), depths\n",
        "    # (t_is_f), and ray directions (ds). See Section 4: r(t) = o + t * d.\n",
        "    r_ts_f = os[..., None, :] + t_is_f[..., :, None] * ds[..., None, :]\n",
        "    return (r_ts_f, t_is_f)\n",
        "\n",
        "\n",
        "def render_radiance_volume(r_ts, ds, chunk_size, F, t_is):\n",
        "    # Use the network (F) to predict colors (c_is) and volume densities (sigma_is) for\n",
        "    # 3D points along rays (r_ts) given the viewing directions (ds) of the rays. See\n",
        "    # Section 3 and Figure 7 in the Supplementary Materials.\n",
        "    r_ts_flat = r_ts.reshape((-1, 3))\n",
        "    ds_rep = ds.unsqueeze(2).repeat(1, 1, r_ts.shape[-2], 1)\n",
        "    ds_flat = ds_rep.reshape((-1, 3))\n",
        "    c_is = []\n",
        "    sigma_is = []\n",
        "    # The network processes batches of inputs to avoid running out of memory.\n",
        "    for chunk_start in range(0, r_ts_flat.shape[0], chunk_size):\n",
        "        r_ts_batch = r_ts_flat[chunk_start : chunk_start + chunk_size]\n",
        "        ds_batch = ds_flat[chunk_start : chunk_start + chunk_size]\n",
        "        preds = F(r_ts_batch, ds_batch)\n",
        "        c_is.append(preds[\"c_is\"])\n",
        "        sigma_is.append(preds[\"sigma_is\"])\n",
        "\n",
        "    c_is = torch.cat(c_is).reshape(r_ts.shape)\n",
        "    sigma_is = torch.cat(sigma_is).reshape(r_ts.shape[:-1])\n",
        "\n",
        "    # Calculate the distances (delta_is) between points along the rays. The differences\n",
        "    # in depths are scaled by the norms of the ray directions to get the final\n",
        "    # distances. See text following Equation (3) in Section 4.\n",
        "    delta_is = t_is[..., 1:] - t_is[..., :-1]\n",
        "    # \"Infinity\". Guarantees last alpha is always one.\n",
        "    one_e_10 = torch.Tensor([1e10]).expand(delta_is[..., :1].shape)\n",
        "    delta_is = torch.cat([delta_is, one_e_10.to(delta_is)], dim=-1)\n",
        "    delta_is = delta_is * ds.norm(dim=-1).unsqueeze(-1)\n",
        "\n",
        "    # Calculate the alphas (alpha_is) of the 3D points using the volume densities\n",
        "    # (sigma_is) and distances between points (delta_is). See text following Equation\n",
        "    # (3) in Section 4 and https://en.wikipedia.org/wiki/Alpha_compositing.\n",
        "    alpha_is = 1.0 - torch.exp(-sigma_is * delta_is)\n",
        "\n",
        "    # Calculate the accumulated transmittances (T_is) along the rays from the alphas\n",
        "    # (alpha_is). See Equation (3) in Section 4. T_i is \"the probability that the ray\n",
        "    # travels from t_n to t_i without hitting any other particle\".\n",
        "    T_is = torch.cumprod(1.0 - alpha_is + 1e-10, -1)\n",
        "    # Guarantees the ray makes it at least to the first step. See:\n",
        "    # https://github.com/bmild/nerf/blob/18b8aebda6700ed659cb27a0c348b737a5f6ab60/run_nerf.py#L142,\n",
        "    # which uses tf.math.cumprod(1.-alpha + 1e-10, axis=-1, exclusive=True).\n",
        "    T_is = torch.roll(T_is, 1, -1)\n",
        "    T_is[..., 0] = 1.0\n",
        "\n",
        "    # Calculate the weights (w_is) for the colors (c_is) along the rays using the\n",
        "    # transmittances (T_is) and alphas (alpha_is). See Equation (5) in Section 5.2:\n",
        "    # w_i = T_i * (1 - exp(-sigma_i * delta_i)).\n",
        "    w_is = T_is * alpha_is\n",
        "\n",
        "    # Calculate the pixel colors (C_rs) for the rays as weighted (w_is) sums of colors\n",
        "    # (c_is). See Equation (5) in Section 5.2: C_c_hat(r) = Σ w_i * c_i.\n",
        "    C_rs = (w_is[..., None] * c_is).sum(dim=-2)\n",
        "\n",
        "    return (C_rs, w_is)\n",
        "\n",
        "\n",
        "def run_one_iter_of_nerf(\n",
        "    ds, N_c, t_i_c_bin_edges, t_i_c_gap, os, chunk_size, F_c, N_f, t_f, F_f\n",
        "):\n",
        "    (r_ts_c, t_is_c) = get_coarse_query_points(ds, N_c, t_i_c_bin_edges, t_i_c_gap, os)\n",
        "    (C_rs_c, w_is_c) = render_radiance_volume(r_ts_c, ds, chunk_size, F_c, t_is_c)\n",
        "\n",
        "    (r_ts_f, t_is_f) = get_fine_query_points(w_is_c, N_f, t_is_c, t_f, os, ds)\n",
        "    (C_rs_f, _) = render_radiance_volume(r_ts_f, ds, chunk_size, F_f, t_is_f)\n",
        "\n",
        "    return (C_rs_c, C_rs_f)\n",
        "\n",
        "\n",
        "class NeRFMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Number of encoding functions for positions. See Section 5.1.\n",
        "        self.L_pos = 10\n",
        "        # Number of encoding functions for viewing directions. See Section 5.1.\n",
        "        self.L_dir = 4\n",
        "        pos_enc_feats = 3 + 3 * 2 * self.L_pos\n",
        "        dir_enc_feats = 3 + 3 * 2 * self.L_dir\n",
        "\n",
        "        in_feats = pos_enc_feats\n",
        "        net_width = 256\n",
        "        early_mlp_layers = 5\n",
        "        early_mlp = []\n",
        "        for layer_idx in range(early_mlp_layers):\n",
        "            early_mlp.append(nn.Linear(in_feats, net_width))\n",
        "            early_mlp.append(nn.ReLU())\n",
        "            in_feats = net_width\n",
        "\n",
        "        self.early_mlp = nn.Sequential(*early_mlp)\n",
        "\n",
        "        in_feats = pos_enc_feats + net_width\n",
        "        late_mlp_layers = 3\n",
        "        late_mlp = []\n",
        "        for layer_idx in range(late_mlp_layers):\n",
        "            late_mlp.append(nn.Linear(in_feats, net_width))\n",
        "            late_mlp.append(nn.ReLU())\n",
        "            in_feats = net_width\n",
        "\n",
        "        self.late_mlp = nn.Sequential(*late_mlp)\n",
        "        self.sigma_layer = nn.Linear(net_width, net_width + 1)\n",
        "        self.pre_final_layer = nn.Sequential(\n",
        "            nn.Linear(dir_enc_feats + net_width, net_width // 2), nn.ReLU()\n",
        "        )\n",
        "        self.final_layer = nn.Sequential(nn.Linear(net_width // 2, 3), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, xs, ds):\n",
        "        # Encode the inputs. See Equation (4) in Section 5.1.\n",
        "        xs_encoded = [xs]\n",
        "        for l_pos in range(self.L_pos):\n",
        "            xs_encoded.append(torch.sin(2 ** l_pos * torch.pi * xs))\n",
        "            xs_encoded.append(torch.cos(2 ** l_pos * torch.pi * xs))\n",
        "\n",
        "        xs_encoded = torch.cat(xs_encoded, dim=-1)\n",
        "\n",
        "        ds = ds / ds.norm(p=2, dim=-1).unsqueeze(-1)\n",
        "        ds_encoded = [ds]\n",
        "        for l_dir in range(self.L_dir):\n",
        "            ds_encoded.append(torch.sin(2 ** l_dir * torch.pi * ds))\n",
        "            ds_encoded.append(torch.cos(2 ** l_dir * torch.pi * ds))\n",
        "\n",
        "        ds_encoded = torch.cat(ds_encoded, dim=-1)\n",
        "\n",
        "        # Use the network to predict colors (c_is) and volume densities (sigma_is) for\n",
        "        # 3D points (xs) along rays given the viewing directions (ds) of the rays. See\n",
        "        # Section 3 and Figure 7 in the Supplementary Materials.\n",
        "        outputs = self.early_mlp(xs_encoded)\n",
        "        outputs = self.late_mlp(torch.cat([xs_encoded, outputs], dim=-1))\n",
        "        outputs = self.sigma_layer(outputs)\n",
        "        sigma_is = torch.relu(outputs[:, 0])\n",
        "        outputs = self.pre_final_layer(torch.cat([ds_encoded, outputs[:, 1:]], dim=-1))\n",
        "        c_is = self.final_layer(outputs)\n",
        "        return {\"c_is\": c_is, \"sigma_is\": sigma_is}"
      ],
      "metadata": {
        "id": "lYJEOcPH76zQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed.\n",
        "seed = 9458\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Initialize coarse and fine MLPs.\n",
        "device = \"cuda:0\"\n",
        "F_c = NeRFMLP().to(device)\n",
        "F_f = NeRFMLP().to(device)\n",
        "# Number of query points passed through the MLP at a time. See: https://github.com/bmild/nerf/blob/18b8aebda6700ed659cb27a0c348b737a5f6ab60/run_nerf.py#L488.\n",
        "chunk_size = 1024 * 32\n",
        "# Number of training rays per iteration. See Section 5.3.\n",
        "batch_img_size = 64\n",
        "n_batch_pix = batch_img_size ** 2\n",
        "\n",
        "# Initialize optimizer. See Section 5.3.\n",
        "lr = 5e-4\n",
        "optimizer = optim.Adam(list(F_c.parameters()) + list(F_f.parameters()), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "# The learning rate decays exponentially. See Section 5.3\n",
        "# See: https://github.com/bmild/nerf/blob/18b8aebda6700ed659cb27a0c348b737a5f6ab60/run_nerf.py#L486.\n",
        "lrate_decay = 250\n",
        "decay_steps = lrate_decay * 1000\n",
        "# See: https://github.com/bmild/nerf/blob/18b8aebda6700ed659cb27a0c348b737a5f6ab60/run_nerf.py#L707.\n",
        "decay_rate = 0.1\n",
        "\n",
        "# Load dataset.\n",
        "data_f = \"66bdbc812bd0a196e194052f3f12cb2e.npz\"\n",
        "data = np.load(data_f)\n",
        "\n",
        "# Set up initial ray origin (init_o) and ray directions (init_ds). These are the\n",
        "# same across samples, we just rotate them based on the orientation of the camera.\n",
        "# See Section 4.\n",
        "images = data[\"images\"] / 255\n",
        "img_size = images.shape[1]\n",
        "xs = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
        "ys = torch.arange(img_size) - (img_size / 2 - 0.5)\n",
        "(xs, ys) = torch.meshgrid(xs, -ys, indexing=\"xy\")\n",
        "focal = float(data[\"focal\"])\n",
        "pixel_coords = torch.stack([xs, ys, torch.full_like(xs, -focal)], dim=-1)\n",
        "# We want the zs to be negative ones, so we divide everything by the focal length\n",
        "# (which is in pixel units).\n",
        "camera_coords = pixel_coords / focal\n",
        "init_ds = camera_coords.to(device)\n",
        "init_o = torch.Tensor(np.array([0, 0, float(data[\"camera_distance\"])])).to(device)\n",
        "\n",
        "# Set up test view.\n",
        "test_idx = 150\n",
        "plt.imshow(images[test_idx])\n",
        "plt.show()\n",
        "test_img = torch.Tensor(images[test_idx]).to(device)\n",
        "poses = data[\"poses\"]\n",
        "test_R = torch.Tensor(poses[test_idx, :3, :3]).to(device)\n",
        "test_ds = torch.einsum(\"ij,hwj->hwi\", test_R, init_ds)\n",
        "test_os = (test_R @ init_o).expand(test_ds.shape)\n",
        "\n",
        "# Initialize volume rendering hyperparameters.\n",
        "# Near bound. See Section 4.\n",
        "t_n = 1.0\n",
        "# Far bound. See Section 4.\n",
        "t_f = 4.0\n",
        "# Number of coarse samples along a ray. See Section 5.3.\n",
        "N_c = 64\n",
        "# Number of fine samples along a ray. See Section 5.3.\n",
        "N_f = 128\n",
        "# Bins used to sample depths along a ray. See Equation (2) in Section 4.\n",
        "t_i_c_gap = (t_f - t_n) / N_c\n",
        "t_i_c_bin_edges = (t_n + torch.arange(N_c) * t_i_c_gap).to(device)\n",
        "\n",
        "# Start training model.\n",
        "train_idxs = np.arange(len(images)) != test_idx\n",
        "images = torch.Tensor(images[train_idxs])\n",
        "poses = torch.Tensor(poses[train_idxs])\n",
        "n_pix = img_size ** 2\n",
        "pixel_ps = torch.full((n_pix,), 1 / n_pix).to(device)\n",
        "psnrs = []\n",
        "iternums = []\n",
        "# See Section 5.3.\n",
        "num_iters = 300000\n",
        "display_every = 100\n",
        "F_c.train()\n",
        "F_f.train()"
      ],
      "metadata": {
        "id": "x2MBw-kt9LGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QL5ewVKG9Ob4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "for i in range(num_iters):\n",
        "    # Sample image and associated pose.\n",
        "    target_img_idx = np.random.randint(images.shape[0])\n",
        "    target_pose = poses[target_img_idx].to(device)\n",
        "    R = target_pose[:3, :3]\n",
        "\n",
        "    # Get rotated ray origins (os) and ray directions (ds). See Section 4.\n",
        "    ds = torch.einsum(\"ij,hwj->hwi\", R, init_ds)\n",
        "    os = (R @ init_o).expand(ds.shape)\n",
        "\n",
        "    # Sample a batch of rays.\n",
        "    pix_idxs = pixel_ps.multinomial(n_batch_pix, False)\n",
        "    pix_idx_rows = pix_idxs // img_size\n",
        "    pix_idx_cols = pix_idxs % img_size\n",
        "    ds_batch = ds[pix_idx_rows, pix_idx_cols].reshape(\n",
        "        batch_img_size, batch_img_size, -1\n",
        "    )\n",
        "    os_batch = os[pix_idx_rows, pix_idx_cols].reshape(\n",
        "        batch_img_size, batch_img_size, -1\n",
        "    )\n",
        "\n",
        "    # Run NeRF.\n",
        "    (C_rs_c, C_rs_f) = run_one_iter_of_nerf(\n",
        "        ds_batch,\n",
        "        N_c,\n",
        "        t_i_c_bin_edges,\n",
        "        t_i_c_gap,\n",
        "        os_batch,\n",
        "        chunk_size,\n",
        "        F_c,\n",
        "        N_f,\n",
        "        t_f,\n",
        "        F_f,\n",
        "    )\n",
        "    target_img = images[target_img_idx].to(device)\n",
        "    target_img_batch = target_img[pix_idx_rows, pix_idx_cols].reshape(C_rs_f.shape)\n",
        "    # Calculate the mean squared error for both the coarse and fine MLP models and\n",
        "    # update the weights. See Equation (6) in Section 5.3.\n",
        "    loss = criterion(C_rs_c, target_img_batch) + criterion(C_rs_f, target_img_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Exponentially decay learning rate. See Section 5.3 and:\n",
        "    # https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/.\n",
        "    for g in optimizer.param_groups:\n",
        "        g[\"lr\"] = lr * decay_rate ** (i / decay_steps)\n",
        "\n",
        "    if i % display_every == 0:\n",
        "        F_c.eval()\n",
        "        F_f.eval()\n",
        "        with torch.no_grad():\n",
        "            (_, C_rs_f) = run_one_iter_of_nerf(\n",
        "                test_ds,\n",
        "                N_c,\n",
        "                t_i_c_bin_edges,\n",
        "                t_i_c_gap,\n",
        "                test_os,\n",
        "                chunk_size,\n",
        "                F_c,\n",
        "                N_f,\n",
        "                t_f,\n",
        "                F_f,\n",
        "            )\n",
        "\n",
        "        loss = criterion(C_rs_f, test_img)\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "        psnr = -10.0 * torch.log10(loss)\n",
        "\n",
        "        psnrs.append(psnr.item())\n",
        "        iternums.append(i)\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(C_rs_f.detach().cpu().numpy())\n",
        "        plt.title(f\"Iteration {i}\")\n",
        "        plt.subplot(122)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        plt.title(\"PSNR\")\n",
        "        plt.show()\n",
        "\n",
        "        F_c.train()\n",
        "        F_f.train()\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "XbCcDAZn9O_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}